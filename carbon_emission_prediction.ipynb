{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "266e4217",
   "metadata": {},
   "source": [
    "# AI for Climate Action: Carbon Emission Prediction Model üåçü§ñ\n",
    "\n",
    "## Week 2 Assignment: Machine Learning Meets UN SDG 13 - Climate Action\n",
    "\n",
    "### Project Overview\n",
    "This project develops a machine learning solution to predict carbon emissions and contribute to **UN Sustainable Development Goal 13: Climate Action**. We'll use supervised learning techniques to forecast CO2 emissions based on economic, industrial, and demographic factors.\n",
    "\n",
    "### Learning Objectives\n",
    "- Apply supervised learning concepts from Week 2\n",
    "- Demonstrate how AI can address global sustainability challenges\n",
    "- Implement ethical AI practices for sustainable development\n",
    "- Create actionable insights for climate policy\n",
    "\n",
    "---\n",
    "*\"AI can be the bridge between innovation and sustainability.\" ‚Äî UN Tech Envoy*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4837b6",
   "metadata": {},
   "source": [
    "## 1. SDG Selection and Problem Definition üéØ\n",
    "\n",
    "### Chosen SDG: SDG 13 - Climate Action\n",
    "\n",
    "**Problem Statement**: Climate change is one of the most pressing global challenges. Accurate prediction of carbon emissions is crucial for:\n",
    "- Setting realistic emission reduction targets\n",
    "- Identifying key factors contributing to emissions  \n",
    "- Developing effective climate policies\n",
    "- Monitoring progress toward carbon neutrality\n",
    "\n",
    "### SDG Targets Addressed:\n",
    "- **Target 13.2**: Integrate climate change measures into national policies and strategies\n",
    "- **Target 13.3**: Improve education and awareness on climate change mitigation\n",
    "\n",
    "### Machine Learning Approach:\n",
    "- **Type**: Supervised Learning (Regression)\n",
    "- **Primary Algorithm**: Random Forest Regression\n",
    "- **Comparison Models**: Linear Regression, XGBoost\n",
    "- **Features**: GDP, Population, Energy Consumption, Industrial Production\n",
    "- **Target Variable**: CO2 Emissions (metric tons per capita)\n",
    "\n",
    "### Expected Impact:\n",
    "This model will help policymakers, organizations, and researchers make data-driven decisions for emission reduction strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f93b76",
   "metadata": {},
   "source": [
    "## 2. Data Collection and Exploration üìä\n",
    "\n",
    "We'll use publicly available data from World Bank and UN databases to ensure transparency and reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29655f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import xgboost as xgb\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs('data', exist_ok=True)\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs('results', exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(\"üìÅ Project directories created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56385a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Synthetic Climate and Economic Data\n",
    "# Note: In a real project, you would load data from World Bank, UN databases, or Kaggle\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Number of countries/regions to simulate\n",
    "n_samples = 200\n",
    "\n",
    "# Generate realistic country data\n",
    "countries = [f\"Country_{i}\" for i in range(1, n_samples + 1)]\n",
    "\n",
    "# Economic indicators\n",
    "gdp_per_capita = np.random.lognormal(mean=9, sigma=1.2, size=n_samples)  # GDP per capita (USD)\n",
    "population = np.random.lognormal(mean=15, sigma=1.5, size=n_samples)     # Population\n",
    "\n",
    "# Energy and industrial factors\n",
    "energy_consumption = np.random.gamma(shape=2, scale=100, size=n_samples)  # Energy consumption per capita\n",
    "renewable_energy_pct = np.random.beta(a=2, b=5, size=n_samples) * 100    # Renewable energy percentage\n",
    "industrial_production = np.random.gamma(shape=3, scale=50, size=n_samples)  # Industrial production index\n",
    "\n",
    "# Environmental factors\n",
    "forest_area_pct = np.random.beta(a=3, b=2, size=n_samples) * 100         # Forest area percentage\n",
    "urbanization_rate = np.random.beta(a=5, b=3, size=n_samples) * 100       # Urban population percentage\n",
    "\n",
    "# Development indicators\n",
    "education_index = np.random.beta(a=8, b=2, size=n_samples)               # Education index (0-1)\n",
    "healthcare_expenditure = np.random.gamma(shape=3, scale=2, size=n_samples)  # Healthcare expenditure % of GDP\n",
    "\n",
    "# Calculate CO2 emissions with realistic relationships\n",
    "co2_emissions = (\n",
    "    0.3 * np.log(gdp_per_capita) +\n",
    "    0.4 * np.log(energy_consumption) +\n",
    "    0.2 * (industrial_production / 100) +\n",
    "    -0.1 * (renewable_energy_pct / 100) +\n",
    "    -0.05 * (forest_area_pct / 100) +\n",
    "    0.1 * (urbanization_rate / 100) +\n",
    "    np.random.normal(0, 0.5, n_samples)  # Add noise\n",
    ")\n",
    "\n",
    "# Ensure realistic bounds\n",
    "co2_emissions = np.clip(co2_emissions, 0.1, 25)  # Typical range: 0.1-25 tons per capita\n",
    "\n",
    "# Create DataFrame\n",
    "data = pd.DataFrame({\n",
    "    'Country': countries,\n",
    "    'GDP_per_capita': gdp_per_capita,\n",
    "    'Population': population,\n",
    "    'Energy_consumption_per_capita': energy_consumption,\n",
    "    'Renewable_energy_pct': renewable_energy_pct,\n",
    "    'Industrial_production_index': industrial_production,\n",
    "    'Forest_area_pct': forest_area_pct,\n",
    "    'Urbanization_rate': urbanization_rate,\n",
    "    'Education_index': education_index,\n",
    "    'Healthcare_expenditure_pct': healthcare_expenditure,\n",
    "    'CO2_emissions_per_capita': co2_emissions\n",
    "})\n",
    "\n",
    "# Save the dataset\n",
    "data.to_csv('data/climate_economic_data.csv', index=False)\n",
    "\n",
    "print(\"üåç Synthetic climate and economic dataset created!\")\n",
    "print(f\"üìä Dataset shape: {data.shape}\")\n",
    "print(\"\\nüìã Dataset Info:\")\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0f54aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory Data Analysis\n",
    "print(\"üìà EXPLORATORY DATA ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Display basic statistics\n",
    "print(\"\\nüìä Dataset Overview:\")\n",
    "print(data.head())\n",
    "\n",
    "print(\"\\nüìà Statistical Summary:\")\n",
    "print(data.describe().round(2))\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"\\n‚ùå Missing values: {data.isnull().sum().sum()}\")\n",
    "\n",
    "# Visualize the distribution of CO2 emissions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# CO2 emissions distribution\n",
    "axes[0, 0].hist(data['CO2_emissions_per_capita'], bins=30, alpha=0.7, color='red')\n",
    "axes[0, 0].set_title('Distribution of CO2 Emissions per Capita')\n",
    "axes[0, 0].set_xlabel('CO2 Emissions (tons per capita)')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "# GDP vs CO2 emissions\n",
    "axes[0, 1].scatter(data['GDP_per_capita'], data['CO2_emissions_per_capita'], alpha=0.6)\n",
    "axes[0, 1].set_title('GDP per Capita vs CO2 Emissions')\n",
    "axes[0, 1].set_xlabel('GDP per Capita (USD)')\n",
    "axes[0, 1].set_ylabel('CO2 Emissions (tons per capita)')\n",
    "\n",
    "# Energy consumption vs CO2 emissions\n",
    "axes[1, 0].scatter(data['Energy_consumption_per_capita'], data['CO2_emissions_per_capita'], alpha=0.6, color='orange')\n",
    "axes[1, 0].set_title('Energy Consumption vs CO2 Emissions')\n",
    "axes[1, 0].set_xlabel('Energy Consumption per Capita')\n",
    "axes[1, 0].set_ylabel('CO2 Emissions (tons per capita)')\n",
    "\n",
    "# Renewable energy vs CO2 emissions\n",
    "axes[1, 1].scatter(data['Renewable_energy_pct'], data['CO2_emissions_per_capita'], alpha=0.6, color='green')\n",
    "axes[1, 1].set_title('Renewable Energy % vs CO2 Emissions')\n",
    "axes[1, 1].set_xlabel('Renewable Energy %')\n",
    "axes[1, 1].set_ylabel('CO2 Emissions (tons per capita)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/exploratory_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Exploratory analysis visualizations saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c461817",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing and Feature Engineering üîß\n",
    "\n",
    "This section handles data cleaning, normalization, and feature creation to optimize model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbcd167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing and Feature Engineering\n",
    "print(\"üîß DATA PREPROCESSING & FEATURE ENGINEERING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create a copy for preprocessing\n",
    "df_processed = data.copy()\n",
    "\n",
    "# Remove country names for modeling (keep for later reference)\n",
    "features_df = df_processed.drop(['Country'], axis=1)\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = features_df.corr()\n",
    "\n",
    "# Visualize correlation heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "sns.heatmap(correlation_matrix, \n",
    "            mask=mask,\n",
    "            annot=True, \n",
    "            cmap='RdYlBu_r', \n",
    "            center=0,\n",
    "            square=True,\n",
    "            fmt='.2f')\n",
    "plt.title('Feature Correlation Matrix', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/correlation_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Feature Engineering - Create new meaningful features\n",
    "df_processed['GDP_per_energy'] = df_processed['GDP_per_capita'] / df_processed['Energy_consumption_per_capita']\n",
    "df_processed['Population_density_proxy'] = df_processed['Population'] * df_processed['Urbanization_rate'] / 100\n",
    "df_processed['Green_development_index'] = (\n",
    "    df_processed['Renewable_energy_pct'] * 0.4 + \n",
    "    df_processed['Forest_area_pct'] * 0.3 + \n",
    "    df_processed['Education_index'] * 100 * 0.3\n",
    ")\n",
    "df_processed['Industrial_intensity'] = df_processed['Industrial_production_index'] / df_processed['GDP_per_capita']\n",
    "\n",
    "# Log transform skewed variables to improve model performance\n",
    "skewed_features = ['GDP_per_capita', 'Population', 'Energy_consumption_per_capita']\n",
    "for feature in skewed_features:\n",
    "    df_processed[f'{feature}_log'] = np.log1p(df_processed[feature])\n",
    "\n",
    "print(\"‚úÖ Feature engineering completed!\")\n",
    "print(f\"üìä New dataset shape: {df_processed.shape}\")\n",
    "print(f\"üî¢ Number of features: {df_processed.shape[1] - 1}\")  # -1 for target variable\n",
    "\n",
    "# Display correlation with target variable\n",
    "target_correlations = correlation_matrix['CO2_emissions_per_capita'].sort_values(key=abs, ascending=False)\n",
    "print(\"\\nüéØ Features most correlated with CO2 emissions:\")\n",
    "print(target_correlations.drop('CO2_emissions_per_capita').head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2180de",
   "metadata": {},
   "source": [
    "## 4. Model Selection and Implementation ü§ñ\n",
    "\n",
    "We'll implement and compare three different machine learning algorithms:\n",
    "1. **Linear Regression** - Simple baseline model\n",
    "2. **Random Forest** - Ensemble method for complex relationships\n",
    "3. **XGBoost** - Gradient boosting for high performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da0974a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Data for Machine Learning\n",
    "print(\"üéØ PREPARING DATA FOR MACHINE LEARNING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Select features for modeling (exclude target variable and original country names)\n",
    "feature_columns = [col for col in df_processed.columns \n",
    "                  if col not in ['CO2_emissions_per_capita', 'Country']]\n",
    "\n",
    "X = df_processed[feature_columns]\n",
    "y = df_processed['CO2_emissions_per_capita']\n",
    "\n",
    "print(f\"üìä Feature matrix shape: {X.shape}\")\n",
    "print(f\"üéØ Target vector shape: {y.shape}\")\n",
    "print(f\"\\nüî¢ Selected features: {feature_columns}\")\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=None\n",
    ")\n",
    "\n",
    "print(f\"\\nüìà Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"üß™ Testing set size: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Scale the features for better model performance\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Save the scaler for future use\n",
    "with open('models/scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "print(\"‚úÖ Data preprocessing completed!\")\n",
    "print(\"üíæ Scaler saved for future predictions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8faea0b9",
   "metadata": {},
   "source": [
    "## 5. Model Training and Hyperparameter Tuning ‚öôÔ∏è\n",
    "\n",
    "We'll train multiple models and optimize their hyperparameters for best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dd17f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training and Hyperparameter Tuning\n",
    "print(\"‚öôÔ∏è MODEL TRAINING & HYPERPARAMETER TUNING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Dictionary to store models and their performance\n",
    "models = {}\n",
    "model_scores = {}\n",
    "\n",
    "# 1. Linear Regression (Baseline Model)\n",
    "print(\"\\nüîµ Training Linear Regression...\")\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "models['Linear Regression'] = lr_model\n",
    "\n",
    "# Cross-validation for Linear Regression\n",
    "lr_cv_scores = cross_val_score(lr_model, X_train_scaled, y_train, cv=5, scoring='r2')\n",
    "model_scores['Linear Regression'] = lr_cv_scores.mean()\n",
    "print(f\"‚úÖ Linear Regression CV R¬≤ Score: {lr_cv_scores.mean():.4f} (+/- {lr_cv_scores.std() * 2:.4f})\")\n",
    "\n",
    "# 2. Random Forest with Hyperparameter Tuning\n",
    "print(\"\\nüå≤ Training Random Forest with GridSearch...\")\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Random Forest hyperparameter grid\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "rf_grid_search = GridSearchCV(\n",
    "    rf_model, rf_param_grid, cv=3, scoring='r2', n_jobs=-1, verbose=1\n",
    ")\n",
    "rf_grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "models['Random Forest'] = rf_grid_search.best_estimator_\n",
    "model_scores['Random Forest'] = rf_grid_search.best_score_\n",
    "print(f\"‚úÖ Random Forest Best CV R¬≤ Score: {rf_grid_search.best_score_:.4f}\")\n",
    "print(f\"üîß Best parameters: {rf_grid_search.best_params_}\")\n",
    "\n",
    "# 3. XGBoost with Hyperparameter Tuning\n",
    "print(\"\\nüöÄ Training XGBoost with GridSearch...\")\n",
    "xgb_model = xgb.XGBRegressor(random_state=42, verbosity=0)\n",
    "\n",
    "# XGBoost hyperparameter grid\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 6, 10],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "xgb_grid_search = GridSearchCV(\n",
    "    xgb_model, xgb_param_grid, cv=3, scoring='r2', n_jobs=-1, verbose=1\n",
    ")\n",
    "xgb_grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "models['XGBoost'] = xgb_grid_search.best_estimator_\n",
    "model_scores['XGBoost'] = xgb_grid_search.best_score_\n",
    "print(f\"‚úÖ XGBoost Best CV R¬≤ Score: {xgb_grid_search.best_score_:.4f}\")\n",
    "print(f\"üîß Best parameters: {xgb_grid_search.best_params_}\")\n",
    "\n",
    "# Display model comparison\n",
    "print(\"\\nüìä MODEL COMPARISON (Cross-Validation R¬≤ Scores):\")\n",
    "print(\"-\" * 50)\n",
    "for model_name, score in model_scores.items():\n",
    "    print(f\"{model_name:15}: {score:.4f}\")\n",
    "\n",
    "# Select best model\n",
    "best_model_name = max(model_scores, key=model_scores.get)\n",
    "best_model = models[best_model_name]\n",
    "print(f\"\\nüèÜ Best Model: {best_model_name} (R¬≤ = {model_scores[best_model_name]:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd1ab0f",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation and Performance Metrics üìà\n",
    "\n",
    "Comprehensive evaluation of our models using multiple regression metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5502034d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation and Performance Metrics\n",
    "print(\"üìà MODEL EVALUATION & PERFORMANCE METRICS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Function to calculate comprehensive metrics\n",
    "def evaluate_model(model, X_test, y_test, model_name):\n",
    "    \"\"\"Calculate comprehensive regression metrics for a model\"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    metrics = {\n",
    "        'MAE': mean_absolute_error(y_test, y_pred),\n",
    "        'MSE': mean_squared_error(y_test, y_pred),\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "        'R¬≤': r2_score(y_test, y_pred)\n",
    "    }\n",
    "    \n",
    "    return y_pred, metrics\n",
    "\n",
    "# Evaluate all models\n",
    "evaluation_results = {}\n",
    "predictions = {}\n",
    "\n",
    "print(\"üß™ Evaluating models on test set...\\n\")\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    y_pred, metrics = evaluate_model(model, X_test_scaled, y_test, model_name)\n",
    "    evaluation_results[model_name] = metrics\n",
    "    predictions[model_name] = y_pred\n",
    "    \n",
    "    print(f\"üìä {model_name} Performance:\")\n",
    "    for metric_name, value in metrics.items():\n",
    "        print(f\"   {metric_name:4}: {value:.4f}\")\n",
    "    print()\n",
    "\n",
    "# Create performance comparison dataframe\n",
    "performance_df = pd.DataFrame(evaluation_results).round(4)\n",
    "print(\"üìã PERFORMANCE COMPARISON TABLE:\")\n",
    "print(performance_df.to_string())\n",
    "\n",
    "# Save evaluation results\n",
    "performance_df.to_csv('results/model_performance.csv')\n",
    "\n",
    "# Feature importance for tree-based models\n",
    "if best_model_name in ['Random Forest', 'XGBoost']:\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_columns,\n",
    "        'importance': best_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(f\"\\nüéØ TOP 10 MOST IMPORTANT FEATURES ({best_model_name}):\")\n",
    "    print(feature_importance.head(10).to_string(index=False))\n",
    "    \n",
    "    # Save feature importance\n",
    "    feature_importance.to_csv('results/feature_importance.csv', index=False)\n",
    "\n",
    "print(\"\\n‚úÖ Model evaluation completed!\")\n",
    "print(\"üíæ Results saved to CSV files!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdeff2fd",
   "metadata": {},
   "source": [
    "## 7. Results Visualization and Interpretation üìä\n",
    "\n",
    "Creating comprehensive visualizations to understand model performance and insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f9aba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results Visualization and Interpretation\n",
    "print(\"üìä CREATING COMPREHENSIVE VISUALIZATIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Model Performance Comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# R¬≤ Score comparison\n",
    "models_list = list(evaluation_results.keys())\n",
    "r2_scores = [evaluation_results[model]['R¬≤'] for model in models_list]\n",
    "\n",
    "axes[0, 0].bar(models_list, r2_scores, color=['lightblue', 'lightgreen', 'lightcoral'])\n",
    "axes[0, 0].set_title('Model R¬≤ Score Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('R¬≤ Score')\n",
    "axes[0, 0].set_ylim(0, 1)\n",
    "for i, v in enumerate(r2_scores):\n",
    "    axes[0, 0].text(i, v + 0.01, f'{v:.3f}', ha='center', fontweight='bold')\n",
    "\n",
    "# RMSE comparison\n",
    "rmse_scores = [evaluation_results[model]['RMSE'] for model in models_list]\n",
    "axes[0, 1].bar(models_list, rmse_scores, color=['lightblue', 'lightgreen', 'lightcoral'])\n",
    "axes[0, 1].set_title('Model RMSE Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('RMSE')\n",
    "for i, v in enumerate(rmse_scores):\n",
    "    axes[0, 1].text(i, v + 0.01, f'{v:.3f}', ha='center', fontweight='bold')\n",
    "\n",
    "# 2. Predictions vs Actual (Best Model)\n",
    "best_predictions = predictions[best_model_name]\n",
    "axes[1, 0].scatter(y_test, best_predictions, alpha=0.7, color='darkgreen')\n",
    "axes[1, 0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "axes[1, 0].set_xlabel('Actual CO2 Emissions')\n",
    "axes[1, 0].set_ylabel('Predicted CO2 Emissions')\n",
    "axes[1, 0].set_title(f'{best_model_name}: Predictions vs Actual', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 3. Residuals plot\n",
    "residuals = y_test - best_predictions\n",
    "axes[1, 1].scatter(best_predictions, residuals, alpha=0.7, color='purple')\n",
    "axes[1, 1].axhline(y=0, color='r', linestyle='--')\n",
    "axes[1, 1].set_xlabel('Predicted CO2 Emissions')\n",
    "axes[1, 1].set_ylabel('Residuals')\n",
    "axes[1, 1].set_title(f'{best_model_name}: Residual Plot', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/model_performance_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 4. Feature Importance Visualization (if available)\n",
    "if best_model_name in ['Random Forest', 'XGBoost']:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_features = feature_importance.head(10)\n",
    "    \n",
    "    bars = plt.barh(range(len(top_features)), top_features['importance'])\n",
    "    plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title(f'Top 10 Most Important Features ({best_model_name})', fontsize=16, fontweight='bold')\n",
    "    plt.gca().invert_yaxis()\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, bar in enumerate(bars):\n",
    "        width = bar.get_width()\n",
    "        plt.text(width + 0.001, bar.get_y() + bar.get_height()/2, \n",
    "                f'{width:.3f}', ha='left', va='center', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# 5. Interactive Plotly visualization\n",
    "fig_interactive = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=('Model Performance (R¬≤)', 'Predictions vs Actual', \n",
    "                   'Feature Importance', 'Residual Distribution'),\n",
    "    specs=[[{\"type\": \"bar\"}, {\"type\": \"scatter\"}],\n",
    "           [{\"type\": \"bar\"}, {\"type\": \"histogram\"}]]\n",
    ")\n",
    "\n",
    "# Model performance\n",
    "fig_interactive.add_trace(\n",
    "    go.Bar(x=models_list, y=r2_scores, name='R¬≤ Score', \n",
    "           marker_color=['lightblue', 'lightgreen', 'lightcoral']),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Predictions vs Actual\n",
    "fig_interactive.add_trace(\n",
    "    go.Scatter(x=y_test, y=best_predictions, mode='markers',\n",
    "               name='Predictions', marker=dict(color='darkgreen', opacity=0.7)),\n",
    "    row=1, col=2\n",
    ")\n",
    "fig_interactive.add_trace(\n",
    "    go.Scatter(x=[y_test.min(), y_test.max()], y=[y_test.min(), y_test.max()],\n",
    "               mode='lines', name='Perfect Prediction', line=dict(color='red', dash='dash')),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Feature importance (if available)\n",
    "if best_model_name in ['Random Forest', 'XGBoost']:\n",
    "    fig_interactive.add_trace(\n",
    "        go.Bar(y=top_features['feature'], x=top_features['importance'],\n",
    "               orientation='h', name='Importance'),\n",
    "        row=2, col=1\n",
    "    )\n",
    "\n",
    "# Residuals distribution\n",
    "fig_interactive.add_trace(\n",
    "    go.Histogram(x=residuals, name='Residuals', nbinsx=20),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "fig_interactive.update_layout(\n",
    "    title=f'Comprehensive Model Analysis - {best_model_name}',\n",
    "    height=800,\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig_interactive.write_html('results/interactive_analysis.html')\n",
    "print(\"‚úÖ Interactive visualization saved as HTML!\")\n",
    "\n",
    "print(\"\\nüéØ KEY INSIGHTS:\")\n",
    "print(f\"‚Ä¢ Best performing model: {best_model_name}\")\n",
    "print(f\"‚Ä¢ Model explains {evaluation_results[best_model_name]['R¬≤']:.1%} of variance in CO2 emissions\")\n",
    "print(f\"‚Ä¢ Average prediction error: {evaluation_results[best_model_name]['MAE']:.2f} tons CO2 per capita\")\n",
    "\n",
    "if best_model_name in ['Random Forest', 'XGBoost']:\n",
    "    top_factor = feature_importance.iloc[0]['feature']\n",
    "    print(f\"‚Ä¢ Most important factor: {top_factor}\")\n",
    "\n",
    "print(\"üíæ All visualizations saved to results/ directory!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39e1ca6",
   "metadata": {},
   "source": [
    "## 8. Ethical Considerations and Bias Analysis ‚öñÔ∏è\n",
    "\n",
    "Critical analysis of potential biases and ethical implications of our carbon emission prediction model."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
